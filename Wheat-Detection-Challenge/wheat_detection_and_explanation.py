# -*- coding: utf-8 -*-
"""Wheat Detection and Explanation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bWfNdnQs1SthQ8pL6gZG8dXOIt5NAbUE
"""

# Commented out IPython magic to ensure Python compatibility.
# importing Libraries 
import pandas as pd
import numpy as np
import os
import re
import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import cv2
import albumentations
import matplotlib.pyplot as plt
import matplotlib.patches as patches
# %matplotlib inline

# reading the train file 
df = pd.read_csv("/content/drive/MyDrive/Wheat_Detection/train.csv")

# Regex for bounding box in the dataframe 
def expand_bbox(x):
    r = np.array(re.findall("([0-9]+[.]?[0-9]*)", x))
    if len(r) == 0:
        r = [-1, -1, -1, -1]
    return r

# adding four individual column for the expansion of bounding boxes+
df['x'] = -1
df['y'] = -1
df['w'] = -1
df['h'] = -1

df[['x', 'y', 'w', 'h']] = np.stack(df['bbox'].apply(lambda x: expand_bbox(x)))

# making all of this as float 
df['x'] = df['x'].astype(float)
df['y'] = df['y'].astype(float)
df['w'] = df['w'].astype(float)
df['h'] = df['h'].astype(float)

#compute x1 and y1 for pascal format [x, y, x1 = x+w, y1 = y+h]
# dropping the bbox from the dataframe 
df.drop(columns=['bbox'], inplace=True)
df['x1'] = df['x'] + df['w']
df['y1'] = df['y'] + df['h']
df['area'] = df['w'] * df['h']

# np.stack(df['bbox'].apply(lambda x: expand_box(x)))
# df[['x', 'y', 'w','h']]
df.head()

# Splitting the Dataset 
# splitting based on image_id
# 80% sample images for training
# 20% sample images for testing
# make new column(folds) where 0 = validation, 1 = train

val_len = int(len(df['image_id'].unique()) * 0.1)

train_set = df['image_id'].unique()[val_len:]
val_set = df['image_id'].unique()[:val_len]

df['folds'] = -1 

df.loc[df.image_id.isin(train_set), 'folds'] = 1
df.loc[df.image_id.isin(val_set), 'folds'] = 0

df["image_id"] = df["image_id"] + ".jpg"

df.head()

from google.colab import drive
drive.mount('/content/drive')

# Create Pytorch dataset class
class WheatDataset(Dataset):
    def __init__(self, dataframe, is_train, transforms = None):
        super(WheatDataset, self).__init__()

        self.df = dataframe
        
        self.transforms = transforms
        
        if is_train:
            self.df = self.df[self.df.folds == 1].reset_index(drop=True)
            self.img_ids = self.df['image_id'].unique()
        else:
            self.df = self.df[self.df.folds == 0].reset_index(drop=True)
            self.img_ids = self.df['image_id'].unique()
        
    def __len__(self):
        return len(self.img_ids)
    
    def __getitem__(self, item):
        
        img_idx = self.img_ids[item]
        
        images = cv2.imread(os.path.join("/content/drive/MyDrive/Wheat_Detection/train", img_idx), cv2.IMREAD_COLOR)
        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)
        
        data = self.df[self.df['image_id'] == img_idx]
        bbox = data[['x','y','x1','y1']].values
        area = data['area'].values
        
        labels = np.ones((bbox.shape[0],))
        is_crowd = np.zeros((bbox.shape[0],))
        taget = {'image':images , 'bboxes': bbox, 'labels': labels,}
        
        if self.transforms:
            transforms = self.transforms(**taget)
            images, bboxs, labels = transforms['image'], transforms['bboxes'], transforms['labels']
            
        images = np.transpose(images, (2, 0, 1)).astype(np.float32)
        
        dict_target = {"boxes": torch.tensor(bboxs, dtype = torch.long), 
                    "area" : torch.tensor(area, dtype = torch.long),
                    "labels": torch.tensor(labels, dtype = torch.long),
                    "iscrowd": torch.tensor(is_crowd, dtype = torch.long),
                   }
        
        return torch.tensor(images, dtype = torch.float), dict_target

IMG_HEIGHT = 512
IMG_WIDTH = 512
IMG_MEAN = (0.5, 0.5, 0.5)
IMG_STD = (0.5, 0.5, 0.5)
BACTH_SIZE = 4

# import numpy as np
# import pandas as pd
# import cv2 as cv 
# from google.colab.patches import cv2_imshow # for image display
# from skimage import io
# from PIL import Image 
# import matplotlib.pylab as plt
# images = io.imread("/content/drive/MyDrive/Wheat Detection/train/00333207f.jpg", 0)
# image_2 = cv.cvtColor(images, cv.COLOR_BGR2RGB)

# # cv2_imshow(image_2)

# h, w, c = image_2.shape
# print('width:  ', w)
# print('height: ', h)
# print('channel:', c)

# # mean, std = cv2_imshow.meanStdDev(image_2)
# mean = np.mean(image_2)
# print(mean)
# std = np.std(image_2)
# print(std)

# image_2.shape[2]

# Augmentations 
# Flip = random vertical, random horizontal or random both vertical and horizontal
# resize to 512x512
# Normalize

train_aug = albumentations.Compose([
            albumentations.Flip(),
            albumentations.Resize(IMG_HEIGHT, IMG_HEIGHT, always_apply = True),            
            albumentations.Normalize(IMG_MEAN, IMG_STD, always_apply = True)
            ], bbox_params = albumentations.BboxParams(format = 'pascal_voc', 
                                                       label_fields=['labels']))

train_data = WheatDataset(dataframe= df , is_train = True, transforms = train_aug)

print("Number of images in training dataset", len(train_data))

# DataLoader 
def collate_fn(batch):
    return list(zip(*batch))

trainloader = DataLoader(train_data,
                           batch_size=BACTH_SIZE,
                           shuffle=False,
                           num_workers=0,
                           collate_fn = collate_fn)

# !pip install augmentation-lib

idx = 18
data = train_data[idx]
img, bbx = data
bbx = bbx['boxes']
img = img / 2 + 0.5    # unnormalize
npimg = img.numpy()
bbx = bbx.numpy()
npimg = np.transpose(npimg,(1, 2, 0))

fig, ax = plt.subplots(1, 1, figsize=(15, 10))

for b in bbx:
        ax.add_patch(
            patches.Rectangle(
            (b[0], b[1]),
            b[2]-b[0],
            b[3]-b[1],
            linewidth=2,
            fill=False,
            color='red'))
    
ax.set_axis_off()
ax.imshow(npimg)

def imshow(img_data, bbx_data, size):
    fig, ax = plt.subplots(nrows = size, ncols = 2, figsize=(30, 30))
    for i, (img, bbx) in enumerate(zip(img_data,bbx_data)):
        img = img / 2 + 0.5
        npimg = img.numpy()
        npimg = np.transpose(npimg,(1, 2, 0))
        ax[i, 0].set_title("Image")
        ax[i, 0].set_axis_off()
        ax[i, 0].imshow(npimg)
        bboxes = bbx['boxes'].numpy()
        for box in bboxes:
            ax[i,1].add_patch(
                patches.Rectangle(
                (box[0], box[1]),
                box[2]-box[0],
                box[3]-box[1],
                linewidth=2,
                fill=False,
                color='red'))
        ax[i, 1].set_axis_off()
        ax[i, 1].set_title("Image with bounding box")
        ax[i, 1].imshow(npimg)

dataiter = iter(trainloader)
data = dataiter.next()
images, bbx = data
imshow(images, bbx, len(images))
plt.show()
plt.close()

