# -*- coding: utf-8 -*-
"""CNN for handwritten numbers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tfa3_lkaQG875dvMtMBSttY5RfAav3Dr
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import tensorflow as tf

# from google.colab import files
# # uploaded = files.upload()

import io
train = pd.read_csv(io.BytesIO(uploaded['train.csv']))
test = pd.read_csv(io.BytesIO(uploaded['test.csv']))

train.head()

y_train = train['label'].astype('float32')
# y_train 
x_train = train.drop(['label'], axis=1).astype('int32')
x_train

x_test = test.astype('float32')

x_train.shape, y_train.shape, x_test.shape

x = sns.countplot(x='label', data=train,palette=['cyan','yellow','pink','b','y','purple','magenta','hotpink','turquoise','lightblue']);
x.set_xlabel("Label",fontsize=30)
x.set_ylabel('Count',fontsize=30)

# Data Normalization 
x_train/= 255.0
x_test/= 255.0

x_train = x_train.values.reshape((-1, 28, 28, 1))
x_test = x_test.values.reshape((-1, 28, 28, 1))
x_train.shape , x_test.shape

# one hot encoding 

from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train, num_classes = 10)
y_train.shape

print(train['label'].head())

y_train[0:7, :]

# now it comes to split the data 
from sklearn.model_selection import train_test_split
x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size = 0.15, random_state=42)


plt.imshow(x_train[1][:,:,0])
plt.title(y_train[1].argmax());

# type(x_train)
x_train.shape

# here comes the deep learning libraries 
from keras.layers import Input,InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D
from keras.layers import AveragePooling2D, MaxPooling2D, Dropout
from keras.models import Sequential,Model
from keras.optimizers import SGD
from keras.callbacks import ModelCheckpoint,LearningRateScheduler
import keras
from keras import backend as K

# Building a CNN model
input_shape = (28,28,1)
x_input = Input(input_shape)

# layer 1
x = Conv2D(64,(3,3),strides=(1,1),name='layer_conv1',padding='same')(x_input)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool1')(x)
# layer 2
x = Conv2D(32,(3,3),strides=(1,1),name='layer_conv2',padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool2')(x)
# layer 3
x = Conv2D(32,(3,3),strides=(1,1),name='conv3',padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2), name='maxPool3')(x)
# fc
x = Flatten()(x)
x = Dense(64,activation ='relu',name='fc0')(x)
x = Dropout(0.25)(x)
x = Dense(32,activation ='relu',name='fc1')(x)
x = Dropout(0.25)(x)
x = Dense(10,activation ='softmax',name='fc2')(x)

conv_model = Model(inputs=x_input, outputs=x, name='Predict')
conv_model.summary()

# Adam optimizer
conv_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
conv_model.fit(x_train, y_train, epochs=20, batch_size=100, validation_data=(x_cv,y_cv))

